{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c81ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:43.092695Z",
     "start_time": "2023-10-19T14:51:42.159126Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, MiniBatchNMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "batch_size = 128\n",
    "init = \"nndsvda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9177493d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:43.101117Z",
     "start_time": "2023-10-19T14:51:43.094565Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5da45c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:43.324979Z",
     "start_time": "2023-10-19T14:51:43.103922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch = fetch_20newsgroups()\n",
    "len(bunch.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9b1bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:43.331543Z",
     "start_time": "2023-10-19T14:51:43.326389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc88328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:44.312824Z",
     "start_time": "2023-10-19T14:51:43.333494Z"
    }
   },
   "outputs": [],
   "source": [
    "data, y = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b858100b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:44.317770Z",
     "start_time": "2023-10-19T14:51:44.314267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62e1047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:44.322312Z",
     "start_time": "2023-10-19T14:51:44.318985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n",
       " \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9622d75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:51:44.327942Z",
     "start_time": "2023-10-19T14:51:44.323878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc233b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:55:35.960891Z",
     "start_time": "2023-10-19T14:55:35.945620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ñaeqa e eqwefdgsgERERdf i not'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "original_string = \"ñáéqa2e/? eqwefdgsgERERdf i'm \\n\\n\\nnot\"\n",
    "\n",
    "def normalize_string(st):\n",
    "    val = re.sub(r\"n't\", \"n \", st)\n",
    "    val = re.sub(r\"'m\", \" \", val)\n",
    "    val = re.sub(r\"'t\", \" \", val)\n",
    "    val = re.sub(r\"_\", \" \", val)\n",
    "    # SUBSTITUTE NON WORD BY WHITESPACE\n",
    "    val = re.sub(r\"\\W\", \" \", val)\n",
    "    # SUBSTITUE ONE OR MORE DIGITS BY WHITESPACE\n",
    "    val = re.sub(r\"\\d+\", \" \", val)\n",
    "    # SUBSTITUTE ONE OR MORE WHITESPACES BY ONE WHITESPACE\n",
    "    val = re.sub(r\"\\s+\", \" \", val)\n",
    "    # SUBSTITUTE ACCENTS\n",
    "    val = re.sub(r\"[áâàä]\", \"a\", val)\n",
    "    val = re.sub(r\"[éêèë]\", \"e\", val)\n",
    "    val = re.sub(r\"[íîìï]\", \"i\", val)\n",
    "    val = re.sub(r\"[óôòö]\", \"o\", val)\n",
    "    val = re.sub(r\"[úûùü]\", \"u\", val)    \n",
    "    \n",
    "    return val\n",
    "\n",
    "normalize_string(original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65be596e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:55:38.451156Z",
     "start_time": "2023-10-19T14:55:36.313108Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"text\": data, \"cat\": y})\n",
    "df['normalized_text'] = (df.text.str.lower()                         \n",
    "                         .map(lambda text: normalize_string(text))\n",
    "                         .str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92084981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:55:38.459016Z",
     "start_time": "2023-10-19T14:55:38.453319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cat</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>17</td>\n",
       "      <td>well i not sure about the story nad it did see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah do you expect people to read the faq etc ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cat  \\\n",
       "0  Well i'm not sure about the story nad it did s...   17   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...    0   \n",
       "\n",
       "                                     normalized_text  \n",
       "0  well i not sure about the story nad it did see...  \n",
       "1  yeah do you expect people to read the faq etc ...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "639245b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T14:55:38.465419Z",
     "start_time": "2023-10-19T14:55:38.460638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey i wasn the one dancing and singing on jan now was i i was roundly ridiculed for my predictions sure they were easy tell that to the other of the people just wait until the see what clinton has planned for their pension funds this one doesn take much thinking either uncle sam needs money bad and pension funds got it well they used to have it turns out the states have been plundering state employee funds for the past years',\n",
       " 'commodore epson homewriter pin printer d s disk drive joysticks mouse lotsa software both games and apps rapid fire joystick adapter about a year old obo',\n",
       " 'i looking for a kawasaki zx engine just the engine no intake exhaust ignition etc preferably in the central texas area but we haven had much luck around here so we ll take whatever we can get please reply via mail or call if you have one or more really need a spare thanx']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.normalized_text.sample(3).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32889f",
   "metadata": {},
   "source": [
    "# Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2f90ab7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:30.151595Z",
     "start_time": "2023-10-19T15:01:29.489090Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "for text in df.normalized_text:\n",
    "    token_list = text.split()\n",
    "    for token in token_list:\n",
    "        if token in vocabulary:\n",
    "            vocabulary[token] += 1\n",
    "        else:\n",
    "            vocabulary[token] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "acbc8662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:32.350244Z",
     "start_time": "2023-10-19T15:01:32.311097Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_series = pd.Series(vocabulary).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3fd70680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:32.678400Z",
     "start_time": "2023-10-19T15:01:32.672322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72026,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ac05a808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:33.644923Z",
     "start_time": "2023-10-19T15:01:33.639667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'ax',\n",
       " 'to',\n",
       " 'a',\n",
       " 'of',\n",
       " 'and',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'for',\n",
       " 'you',\n",
       " 's',\n",
       " 'on',\n",
       " 'this',\n",
       " 'be',\n",
       " 'are',\n",
       " 'have',\n",
       " 'not',\n",
       " 'with',\n",
       " 'as',\n",
       " 'm',\n",
       " 'or',\n",
       " 'x',\n",
       " 'but',\n",
       " 'was',\n",
       " 'if',\n",
       " 'they',\n",
       " 'from']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_series.head(30).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eec80e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:40.008692Z",
     "start_time": "2023-10-19T15:01:40.005796Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_stopwords = [\n",
    "    'ax', 'x', 'q', 'w', 'f', 'g', 'p', 'r', 'b', 'u', 'v', 'would', 'c', 'e',\n",
    "    'n', 'l', 'k', 'z', 'get', 'also', 'h', 'j'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "28e5b2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:43.760437Z",
     "start_time": "2023-10-19T15:01:43.744410Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'aren ',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'couldn ',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didn ',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesn ',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'don ',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'hadn ',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'hasn ',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'haven ',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'isn ',\n",
       " 'it',\n",
       " 'it s',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'mightn ',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'mustn ',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'needn ',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'shan ',\n",
       " 'she',\n",
       " 'she s',\n",
       " 'should',\n",
       " 'should ve',\n",
       " 'shouldn',\n",
       " 'shouldn ',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'that ll',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'wasn ',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'weren ',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'won ',\n",
       " 'wouldn',\n",
       " 'wouldn ',\n",
       " 'y',\n",
       " 'you',\n",
       " 'you d',\n",
       " 'you ll',\n",
       " 'you re',\n",
       " 'you ve',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "generic_stopwords = nltk.corpus.stopwords.words('english')\n",
    "generic_stopwords = set([normalize_string(word) for word in generic_stopwords])\n",
    "generic_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cae17da1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:47.046001Z",
     "start_time": "2023-10-19T15:01:47.041286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = generic_stopwords.union(corpus_stopwords)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "15d39267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:49.153639Z",
     "start_time": "2023-10-19T15:01:49.149179Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_from_text(text, stopwords_set):\n",
    "    val = ' '.join([word for word in text.split() \n",
    "                    if word not in stopwords_set])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d41e36e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:49.815423Z",
     "start_time": "2023-10-19T15:01:49.524307Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_normalized_text'] = \\\n",
    "df.normalized_text.map(lambda text: \n",
    "                       remove_stopwords_from_text(text=text, \n",
    "                                                  stopwords_set=stop_words))\n",
    "df = df.loc[df.clean_normalized_text.str.len().gt(140)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "37723491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:51.118297Z",
     "start_time": "2023-10-19T15:01:51.111662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8427, 4)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "91f4c06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:01:51.691112Z",
     "start_time": "2023-10-19T15:01:51.683741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['want create single line text widget entering small amount text want fixed width horizontal scrollbar scrolls automatically user types order keep insertion point visible trying two problems addition horizontal scrollbar make text widget taller instead seems cover part text scrollbar scroll automatically user types text order keep insertion point visible help appreciated mike',\n",
       " 'sorry waste bandwidth anyone know software mail order company called software unlimited ordered software charged credit card never send package call many times nobody answer phone check computer shoppers found advertise anymore know still business know contact please tell thank much',\n",
       " 'heavy traffic slow bit mostly buffer zone front balance minimal buffer behind often find jerk behind notice traffic moving faster lanes switch one pass fine keep better eye jerk behind looking ahead rather front splitting attention ahead mirrors pretty damned complicated make back motion hand arm second third time even braindead cager backs back find way hell cager either psychotic drunk complete asshole case want anywhere near especially front']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_normalized_text.sample(3).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ea8f3",
   "metadata": {},
   "source": [
    "## Lematización\n",
    "\n",
    "- esperaba -> esperar\n",
    "- horas -> hora\n",
    "- estuviéseis -> estar\n",
    "- ...\n",
    "\n",
    "\n",
    "\"buenos dias que tal esperaba que pudiera atenderme\"\n",
    "\n",
    "LEMATIZAMOS:\n",
    "\n",
    "\"buen dia que tal esperar que poder atender\"\n",
    "\n",
    "\n",
    "`pip install spacy`\n",
    "\n",
    "después descargamos el modelo en inglés más pequeño\n",
    "\n",
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43a52adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T15:09:01.736606Z",
     "start_time": "2023-10-19T15:09:01.268264Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607e9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
